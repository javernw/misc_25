from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, datediff, sum as _sum
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.master("local").appName("MirrorTrading").getOrCreate()

# Sample schema: customer_id, security_id, transaction_type (buy/sell), amount, date
# You can replace this with loading your data
data = [
    ("customer_a", "security_1", "buy", 1000, "2025-02-01"),
    ("customer_b", "security_1", "sell", 950, "2025-02-02"),
    ("customer_a", "security_1", "buy", 1200, "2025-02-04"),
    ("customer_b", "security_1", "sell", 1100, "2025-02-05"),
    # Add more sample data here
]

# Create DataFrame from data
df = spark.createDataFrame(data, ["customer_id", "security_id", "transaction_type", "amount", "date"])

# Convert date column to proper date format
df = df.withColumn("date", unix_timestamp("date", "yyyy-MM-dd").cast("timestamp"))

# Create a self-join to check for mirror trading conditions
# Join on security_id, but customers must not be the same
df_joined = df.alias("df1").join(df.alias("df2"),
                                 (df["security_id"] == df["df2.security_id"]) &
                                 (df["customer_id"] != df["df2.customer_id"]),
                                 "inner")

# Add a column to calculate the difference in dates for the same security_id
df_joined = df_joined.withColumn("date_diff", datediff("df2.date", "df1.date"))

# Filter the rows where the date difference is within 5 days
df_joined = df_joined.filter((col("date_diff") >= 0) & (col("date_diff") <= 5))

# Filter for opposite transaction types (buy/sell)
df_joined = df_joined.filter(((col("df1.transaction_type") == "buy") & (col("df2.transaction_type") == "sell")) |
                             ((col("df1.transaction_type") == "sell") & (col("df2.transaction_type") == "buy")))

# Calculate the notional ratio
df_joined = df_joined.withColumn("notional_ratio", 
                                 col("df1.amount") / col("df2.amount"))

# Filter for notional ratio between 90% and 110%
df_joined = df_joined.filter((col("notional_ratio") >= 0.9) & (col("notional_ratio") <= 1.1))

# For each customer, get the aggregate sum of amounts (buy or sell) within 5 days
df_aggregated = df_joined.groupBy("df1.customer_id", "df1.security_id", "df1.date") \
    .agg(
        _sum("df1.amount").alias("buy_amount"),
        _sum("df2.amount").alias("sell_amount")
    )

# Show the results
df_aggregated.show()

# Stop Spark session
spark.stop()


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, sum as _sum, unix_timestamp
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    # More rows can be added here
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to timestamp for easier manipulation
df = df.withColumn("trade_date", unix_timestamp(col("trade_date"), "yyyy-MM-dd").cast("timestamp"))

# Define window for 5-day lookback per customer and security
window_spec = Window.partitionBy("security_id", "customer_id").orderBy("trade_date")

# Add column for the 5-day lookback for aggregate sum
df = df.withColumn("five_day_window", F.sum("notional_value").over(window_spec.rangeBetween(-4, 0)))

# Self-join the DataFrame to compare each customer's trades with the other
mirror_trading_df = df.alias("df_a") \
    .join(df.alias("df_b"), (col("df_a.security_id") == col("df_b.security_id")) & 
          (col("df_a.customer_id") != col("df_b.customer_id")), "inner") \
    .filter((col("df_a.trade_date") <= col("df_b.trade_date")) & 
            (col("df_a.trade_date") >= (col("df_b.trade_date") - F.expr("INTERVAL 5 DAYS"))))

# Compute the absolute difference in notional value between customers A and B
mirror_trading_df = mirror_trading_df.withColumn("notional_diff", 
                                                 (col("df_a.notional_value") / col("df_b.notional_value")).cast("double"))

# Check if the notional value difference is within 90%-110% range
mirror_trading_df = mirror_trading_df.withColumn("notional_check", 
                                                 when((col("notional_diff") >= 0.9) & (col("notional_diff") <= 1.1), 1).otherwise(0))

# Filter for trades with 3 or more instances within the 30-day window
window_spec_30_day = Window.partitionBy("security_id", "customer_id").rangeBetween(-30 * 24 * 60 * 60, 0)
mirror_trading_df = mirror_trading_df.withColumn("count_within_30_days", 
                                                 F.count("df_a.trade_date").over(window_spec_30_day))

# Filter the results to find instances where customers are trading in opposite directions (buy/sell)
mirror_trading_df = mirror_trading_df.filter((col("df_a.trade_type") != col("df_b.trade_type")) & 
                                              (col("notional_check") == 1) & 
                                              (col("count_within_30_days") >= 3))

# Select relevant columns and show the results
result_df = mirror_trading_df.select("df_a.customer_id", "df_a.trade_date", "df_a.security_id", 
                                     "df_a.trade_type", "df_a.notional_value", "df_b.customer_id", 
                                     "df_b.trade_date", "df_b.trade_type", "df_b.notional_value")

# Display the result
result_df.show(truncate=False)


condition = (
    (col("currencyStatus1") != col("currencyStatus2")) & 
    (
        ((col("buyorsell2_1") == "SELL") | (col("buyorsell2_2") == "SELL")) & 
        ((col("currencyStatus1") == "restricted") | (col("currencyStatus2") == "restricted"))
    )
)


from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    ("customer_a", "2025-03-10", "SEC1", "buy", 1030),
    ("customer_b", "2025-03-10", "SEC1", "sell", 1010),
    ("customer_a", "2025-03-15", "SEC1", "buy", 1040),
    ("customer_b", "2025-03-15", "SEC1", "sell", 1025),
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to date format for comparison
df = df.withColumn("trade_date", F.to_date("trade_date", "yyyy-MM-dd"))

# Self-join on security_id to compare trades between different customers
mirror_trading_df = df.alias("df_a") \
    .join(df.alias("df_b"), 
          (F.col("df_a.security_id") == F.col("df_b.security_id")) & 
          (F.col("df_a.customer_id") != F.col("df_b.customer_id")), 
          "inner") \
    .filter(
        (F.col("df_a.trade_date") >= F.col("df_b.trade_date")) & 
        (F.col("df_a.trade_date") <= F.col("df_b.trade_date") + F.expr("INTERVAL 5 DAYS"))
    ) \
    .filter(F.col("df_a.trade_type") != F.col("df_b.trade_type"))  # Buy vs Sell

# Compute the ratio of notional values
mirror_trading_df = mirror_trading_df.withColumn("notional_ratio", 
                                                 (F.col("df_a.notional_value") / F.col("df_b.notional_value")).cast("double"))

# Check if the notional value ratio is within 90%-110%
mirror_trading_df = mirror_trading_df.filter(
    (F.col("notional_ratio") >= 0.9) & (F.col("notional_ratio") <= 1.1)
)

# Count trades per customer pair within 30 days
trade_count_df = mirror_trading_df.groupBy("df_a.security_id", "df_a.customer_id", "df_b.customer_id") \
    .agg(F.count("*").alias("mirror_trade_count"))

# Filter for customer pairs with at least 3 mirror trading instances
trade_count_df = trade_count_df.filter(F.col("mirror_trade_count") >= 3)

# Join back to get full details of those trades
final_result_df = mirror_trading_df.join(trade_count_df, 
                                         on=["security_id", "df_a.customer_id", "df_b.customer_id"], 
                                         how="inner")

# Select relevant columns for final output
final_result_df = final_result_df.select(
    "df_a.customer_id", "df_a.trade_date", "df_a.security_id",
    "df_a.trade_type", "df_a.notional_value", "df_b.customer_id",
    "df_b.trade_date", "df_b.trade_type", "df_b.notional_value",
    "mirror_trade_count"
)

# Show results
final_result_df.show(truncate=False)


from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample Data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    ("customer_a", "2025-03-10", "SEC1", "buy", 1030),
    ("customer_b", "2025-03-10", "SEC1", "sell", 1010),
    ("customer_a", "2025-03-15", "SEC1", "buy", 1040),
    ("customer_b", "2025-03-15", "SEC1", "sell", 1025),
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to date format
df = df.withColumn("trade_date", F.to_date("trade_date", "yyyy-MM-dd"))

# Aggregate sum of notional values within a 5-day rolling window for each customer & security
df_agg = df.alias("df1").join(
    df.alias("df2"),
    (F.col("df1.security_id") == F.col("df2.security_id")) &
    (F.col("df1.customer_id") == F.col("df2.customer_id")) &
    (F.col("df1.trade_date") >= F.col("df2.trade_date")) &
    (F.col("df1.trade_date") <= F.col("df2.trade_date") + F.expr("INTERVAL 5 DAYS")),
    "inner"
).groupBy("df1.customer_id", "df1.trade_date", "df1.security_id", "df1.trade_type") \
 .agg(F.sum("df1.notional_value").alias("aggregate_notional_value"))

# Self-join on security_id to find mirror trades within the 5-day window
mirror_trading_df = df_agg.alias("df_a") \
    .join(df_agg.alias("df_b"), 
          (F.col("df_a.security_id") == F.col("df_b.security_id")) & 
          (F.col("df_a.customer_id") != F.col("df_b.customer_id")), 
          "inner") \
    .filter(
        (F.col("df_a.trade_date") >= F.col("df_b.trade_date")) & 
        (F.col("df_a.trade_date") <= F.col("df_b.trade_date") + F.expr("INTERVAL 5 DAYS"))
    ) \
    .filter(F.col("df_a.trade_type") != F.col("df_b.trade_type"))  # Buy vs Sell

# Compute the ratio of aggregate notional values
mirror_trading_df = mirror_trading_df.withColumn("notional_ratio", 
    (F.col("df_a.aggregate_notional_value") / F.col("df_b.aggregate_notional_value")).cast("double"))

# Check if the notional value ratio is within 90%-110%
mirror_trading_df = mirror_trading_df.filter(
    (F.col("notional_ratio") >= 0.9) & (F.col("notional_ratio") <= 1.1)
)

# Count trades per customer pair within 30 days
trade_count_df = mirror_trading_df.groupBy("df_a.security_id", "df_a.customer_id", "df_b.customer_id") \
    .agg(F.count("*").alias("mirror_trade_count"))

# Filter for customer pairs with at least 3 mirror trading instances
trade_count_df = trade_count_df.filter(F.col("mirror_trade_count") >= 3)

# Join back to get full details of those trades
final_result_df = mirror_trading_df.join(trade_count_df, 
                                         on=["security_id", "df_a.customer_id", "df_b.customer_id"], 
                                         how="inner")

# Select relevant columns for final output
final_result_df = final_result_df.select(
    "df_a.customer_id", "df_a.trade_date", "df_a.security_id",
    "df_a.trade_type", "df_a.aggregate_notional_value", "df_b.customer_id",
    "df_b.trade_date", "df_b.trade_type", "df_b.aggregate_notional_value",
    "mirror_trade_count"
)

# Show results
final_result_df.show(truncate=False)

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark Session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingLoop").getOrCreate()

# Sample list of date ranges (each tuple represents a start and end date)
date_ranges = [
    ("2024-08-18", "2024-08-23"),
    ("2024-08-25", "2024-08-30"),
    ("2024-09-01", "2024-09-05")
]

# Placeholder list to store results from each iteration
results = []

# Loop through each date range and execute the query
for start_date, end_date in date_ranges:
    # Filter data within the current date range
    mt_eval_df = df.filter(F.col("security_id") == "H574748") \
                   .filter(F.col("businessdate").between(start_date, end_date)) \
                   .filter(F.col("IDAgg").isin([1010, 1005])) \
                   .groupBy("IDAgg", "security_id", "buyorsell2") \
                   .agg(F.sum("amountusd").alias("agg_notional_value"))

    # Self-join for mirror trade evaluation
    mt_data_df = mt_eval_df.alias("MT_a").join(
        mt_eval_df.alias("MT_b"),
        (F.col("MT_a.security_id") == F.col("MT_b.security_id")) &
        (F.col("MT_a.IDAgg") != F.col("MT_b.IDAgg")),
        "inner"
    )

    # Append the result to the list
    results.append(mt_data_df)

# Union all results into one DataFrame
final_df = results[0]  # Start with the first result
for df in results[1:]:
    final_df = final_df.union(df)  # Append subsequent results

# Show final output
final_df.show()



from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark Session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingLoop").getOrCreate()

# List of customer pairs and their respective unique date ranges
customer_pairs = ["cust1_cust2", "cust3_cust4", "cust5_cust6"]

# Assign unique date ranges to each customer pair dynamically
customer_date_ranges = {
    tuple(pair.split("_")): date_range for pair, date_range in zip(
        customer_pairs, 
        [("2024-08-18", "2024-08-23"), ("2024-09-01", "2024-09-05"), ("2024-09-20", "2024-09-25")]  # Unique date ranges
    )
}

# Placeholder list to store results
results = []

# Loop through each customer pair and their respective date range
for (customer1, customer2), (start_date, end_date) in customer_date_ranges.items():
    
    # Filter data within the current customer pair and their date range
    mt_eval_df = df.filter(
        (F.col("customer_id").isin([customer1, customer2])) &  # Match the customer pair
        (F.col("businessdate").between(start_date, end_date))  # Match their assigned date range
    ).groupBy("customer_id", "security_id", "buyorsell2") \
     .agg(F.sum("amountusd").alias("agg_notional_value"))

    # Self-join for mirror trade evaluation within the same security
    mt_data_df = mt_eval_df.alias("MT_a").join(
        mt_eval_df.alias("MT_b"),
        (F.col("MT_a.security_id") == F.col("MT_b.security_id")) &
        (F.col("MT_a.customer_id") != F.col("MT_b.customer_id")),  # Ensuring it's a trade between different customers


        "inner"
    ).withColumn("customer_pair", F.lit(f"{customer1} & {customer2}")) \
     .withColumn("date_range", F.lit(f"{start_date} to {end_date}"))

    # Append the result to the list
    results.append(mt_data_df)

# Union all results into one DataFrame if results exist
if results:
    final_df = results[0]  # Start with the first result
    for df in results[1:]:
        final_df = final_df.union(df)  # Append subsequent results

    # Show final output
    final_df.show(truncate=False)
else:
    print("No matching data found for the given customer pairs and date ranges.")




from pyspark.sql.functions import col, lit, concat, when, sum as _sum
from datetime import datetime, timedelta

def find_mirror_trades(customer1, customer2, date1, date2, df, sec_list=None):
    # Prepare empty output DataFrame with same schema
    final_df = spark.createDataFrame([], df.schema)

    # Convert to datetime
    start_date = datetime.strptime(date1, "%Y-%m-%d")
    cut_off_date = datetime.strptime(date2, "%Y-%m-%d")
    end_date = start_date + timedelta(days=5)

    # Loop until 5-day windows pass the cutoff date
    while end_date <= cut_off_date:
        current_start_str = start_date.strftime("%Y-%m-%d")
        current_end_str = end_date.strftime("%Y-%m-%d")

        # Filter for relevant trades
        mt_eval_df = df.filter(col("customer_id").isin([customer1, customer2])) \
                       .filter(col("security_id").isin(sec_list)) \
                       .filter(col("businessdate").between(current_start_str, current_end_str)) \
                       .groupBy("customer_id", "security_id", "buyorsell2") \
                       .agg(_sum("amountusd").alias("agg_notional_value"))

        # Self-join to match trades in opposite directions
        mt_data_df = mt_eval_df.alias("a").join(
            mt_eval_df.alias("b"),
            (col("a.security_id") == col("b.security_id")) &
            (col("a.customer_id") != col("b.customer_id")),
            "inner"
        )

        # Rename columns to keep both sides of the trade
        for col_name in ["customer_id", "security_id", "buyorsell2", "agg_notional_value"]:
            mt_data_df = mt_data_df.withColumnRenamed(f"a.{col_name}", f"{col_name}_1") \
                                   .withColumnRenamed(f"b.{col_name}", f"{col_name}_2")

        # Compute notional value ratios
        mt_data_df = mt_data_df.withColumn("ratio_diff1", (col("agg_notional_value_1") / col("agg_notional_value_2")).cast("double")) \
                               .withColumn("ratio_diff2", (col("agg_notional_value_2") / col("agg_notional_value_1")).cast("double")) \
                               .withColumn("ratio_check", when((col("ratio_diff1") >= 0.9) & (col("ratio_diff1") <= 1.1), 1).otherwise(0)) \
                               .withColumn("ratio_check2", when((col("ratio_diff2") >= 0.9) & (col("ratio_diff2") <= 1.1), 1).otherwise(0))

        # Filter for valid mirror trades
        mt_data_df = mt_data_df.filter(
            ((col("ratio_check") == 1) | (col("ratio_check2") == 1)) &
            (col("buyorsell2_1") != col("buyorsell2_2")) &
            ((col("agg_notional_value_1") > 5000) | (col("agg_notional_value_2") > 5000))
        )

        # Add rolling period label
        mt_data_df = mt_data_df.withColumn("rolling_period", concat(lit(current_start_str), lit("_to_"), lit(current_end_str)))

        # Union to final result
        final_df = final_df.unionByName(mt_data_df, allowMissingColumns=True)

        # Move the window forward by 1 day
        start_date += timedelta(days=1)
        end_date += timedelta(days=1)

    # Drop exact duplicates on matched keys
    final_df = final_df.dropDuplicates(subset=[
        "customer_id_1", "customer_id_2", "security_id_1", "buyorsell2_1", "agg_notional_value_1",
        "security_id_2", "buyorsell2_2", "agg_notional_value_2"
    ])

    # Count matched instances per pair & security
    instance_df = final_df.groupBy("customer_id_1", "customer_id_2", "security_id_1").count() \
                          .withColumnRenamed("count", "instance_size") \
                          .withColumnRenamed("customer_id_1", "cid1") \
                          .withColumnRenamed("customer_id_2", "cid2") \
                          .withColumnRenamed("security_id_1", "secid")

    # Join instance size back to final DataFrame
    final_df = final_df.join(
        instance_df,
        (final_df.customer_id_1 == instance_df.cid1) &
        (final_df.customer_id_2 == instance_df.cid2) &
        (final_df.security_id_1 == instance_df.secid),
        "left"
    ).drop("cid1", "cid2", "secid")

    # Final filters
    final_df = final_df.dropDuplicates()
    final_df = final_df.filter(col("instance_size") >= 3)
    final_df = final_df.filter(col("buyorsell2_1") == "BUY")

    return final_df


from pyspark.sql.functions import col, collect_set, collect_list
from datetime import datetime

def extra_stuff(pair):
    # Get unique list of security IDs for the pair
    securityids = (
        MT_ui.filter(col("customerPair") == pair)
             .select("esmp")
             .dropna()
             .agg(collect_set("esmp").alias("sec_ids"))
             .first()["sec_ids"]
    )

    # Get sorted rolling period dates (start of 5-day windows)
    rp_start = (
        MT_ui.filter(col("customerPair") == pair)
             .select("rollingPeriod")
             .dropna()
             .distinct()
             .rdd.map(lambda row: row["rollingPeriod"].split("_to_")[1])
             .collect()
    )
    rp_start = sorted(set(rp_start))

    # Get sorted list of business dates
    rp_end = (
        MT_ui.filter(col("customerPair") == pair)
             .select("businessDate")
             .dropna()
             .distinct()
             .rdd.map(lambda row: row["businessDate"])
             .collect()
    )
    rp_end = sorted(set(rp_end))

    # Return date range and securities
    if rp_start and rp_end:
        return [rp_start[0], rp_end[-1]], securityids
    else:
        return None, None




from pyspark.sql.functions import col, lit, concat, when, sum as _sum
from datetime import datetime, timedelta

def find_mirror_trades(customer1, customer2, date1, date2, df, sec_list=None):
    # Prepare empty output DataFrame with same schema
    final_df = spark.createDataFrame([], df.schema)

    # Step 1: Get actual business dates where trades occurred for these customers
    available_dates = (
        df.filter(col("customer_id").isin([customer1, customer2]))
          .filter(col("businessdate").between(date1, date2))
          .select("businessdate")
          .distinct()
          .rdd.map(lambda row: row["businessdate"])
          .collect()
    )

    # Convert to datetime and sort
    available_dates = sorted([datetime.strptime(str(d), "%Y-%m-%d") for d in available_dates])

    for end_date in available_dates:
        start_date = end_date - timedelta(days=5)

        current_start_str = start_date.strftime("%Y-%m-%d")
        current_end_str = end_date.strftime("%Y-%m-%d")

        # Step 2: Filter relevant trades within rolling window
        mt_eval_df = df.filter(col("customer_id").isin([customer1, customer2])) \
                       .filter(col("security_id").isin(sec_list)) \
                       .filter(col("businessdate").between(current_start_str, current_end_str)) \
                       .groupBy("customer_id", "security_id", "buyorsell2") \
                       .agg(_sum("amountusd").alias("agg_notional_value"))

        # Step 3: Self-join on same security, opposite customer
        mt_data_df = mt_eval_df.alias("a").join(
            mt_eval_df.alias("b"),
            (col("a.security_id") == col("b.security_id")) &
            (col("a.customer_id") != col("b.customer_id")),
            "inner"
        ).select(
            col("a.customer_id").alias("customer_id_1"),
            col("a.security_id").alias("security_id_1"),
            col("a.buyorsell2").alias("buyorsell2_1"),
            col("a.agg_notional_value").alias("agg_notional_value_1"),
            col("b.customer_id").alias("customer_id_2"),
            col("b.security_id").alias("security_id_2"),
            col("b.buyorsell2").alias("buyorsell2_2"),
            col("b.agg_notional_value").alias("agg_notional_value_2")
        )

        # Step 4: Ratio logic
        mt_data_df = mt_data_df.withColumn("ratio_diff1", (col("agg_notional_value_1") / col("agg_notional_value_2")).cast("double")) \
                               .withColumn("ratio_diff2", (col("agg_notional_value_2") / col("agg_notional_value_1")).cast("double")) \
                               .withColumn("ratio_check", when((col("ratio_diff1") >= 0.9) & (col("ratio_diff1") <= 1.1), 1).otherwise(0)) \
                               .withColumn("ratio_check2", when((col("ratio_diff2") >= 0.9) & (col("ratio_diff2") <= 1.1), 1).otherwise(0))

        # Step 5: Filter valid mirror trades
        mt_data_df = mt_data_df.filter(
            ((col("ratio_check") == 1) | (col("ratio_check2") == 1)) &
            (col("buyorsell2_1") != col("buyorsell2_2")) &
            ((col("agg_notional_value_1") > 5000) | (col("agg_notional_value_2") > 5000))
        )

        # Step 6: Add rolling period label
        mt_data_df = mt_data_df.withColumn("rolling_period", concat(lit(current_start_str), lit("_to_"), lit(current_end_str)))

        # Step 7: Append to final DataFrame
        final_df = final_df.unionByName(mt_data_df, allowMissingColumns=True)

    # Step 8: Deduplicate and summarize
    final_df = final_df.dropDuplicates(subset=[
        "customer_id_1", "customer_id_2", "security_id_1", "buyorsell2_1", "agg_notional_value_1",
        "security_id_2", "buyorsell2_2", "agg_notional_value_2"
    ])

    # Count number of mirror trade instances per pair
    instance_df = final_df.groupBy("customer_id_1", "customer_id_2").count() \
                          .withColumnRenamed("count", "instance_size") \
                          .withColumnRenamed("customer_id_1", "cid1") \
                          .withColumnRenamed("customer_id_2", "cid2")

    final_df = final_df.join(
        instance_df,
        (final_df.customer_id_1 == instance_df.cid1) &
        (final_df.customer_id_2 == instance_df.cid2),
        "left"
    ).drop("cid1", "cid2")

    # Final filter and cleanup
    final_df = final_df.dropDuplicates()
    final_df = final_df.filter(col("instance_size") >= 3)
    final_df = final_df.filter(col("buyorsell2_1") == "BUY")

    return final_df


# Step 2: Get trades in the current 5-day window
        window_trades = df_filtered.filter(col("businessdate").between(current_start_str, current_end_str))

        # Step 3: Get distinct (customer_id, security_id) pairs from the 5-day window
        cust_sec_pairs = window_trades.select("customer_id", "security_id").distinct()

        # Step 4: Get (customer_id, security_id) with BUY or SELL on end_date
        trades_on_end = df_filtered.filter(col("businessdate") == current_end_str) \
                                   .filter(col("buyorsell2").isin(["BUY", "SELL"])) \
                                   .select("customer_id", "security_id").distinct()

        # Step 5: Ensure overlap
        valid_end_day_trades = cust_sec_pairs.join(
            trades_on_end,
            on=["customer_id", "security_id"],
            how="inner"
        )

        if valid_end_day_trades.count() == 0:
            continue  # skip noisy window

        # Step 6: Aggregate notional values within the 5-day window
        mt_eval_df = window_trades.groupBy("customer_id", "security_id", "buyorsell2") \
                                  .agg(_sum("amountusd").alias("agg_notional_value"))




from pyspark.sql.functions import col, lit, concat, when, sum as _sum, round, row_number, split
from pyspark.sql.window import Window
from datetime import datetime, timedelta

def find_mirror_trades(customer1, customer2, date1, date2, df):
    """
    Detects mirror trades between two customers using 5-day rolling windows.
    Filters out patterns where no valid trade happened on the end date of the window.
    """
    df_filtered = df.filter(col("customer_id").isin([customer1, customer2])).cache()

    # Get rolling end dates (businessdate)
    available_dates = (
        df_filtered.filter(col("businessdate").between(date1, date2))
                   .select("businessdate").distinct()
                   .rdd.map(lambda row: datetime.strptime(str(row["businessdate"]), "%Y-%m-%d"))
                   .collect()
    )
    available_dates = sorted(available_dates)
    min_allowed_start = datetime.strptime("2024-08-03", "%Y-%m-%d")

    final_df = spark.createDataFrame([], df.schema)

    for end_date in available_dates:
        start_date = end_date - timedelta(days=5)
        if start_date < min_allowed_start:
            continue

        current_start_str = start_date.strftime("%Y-%m-%d")
        current_end_str = end_date.strftime("%Y-%m-%d")

        window_trades = df_filtered.filter(col("businessdate").between(current_start_str, current_end_str))

        mt_eval_df = window_trades.groupBy("customer_id", "security_id", "buyorsell2") \
                                  .agg(_sum("amountusd").alias("agg_notional_value"))

        mt_data_df = mt_eval_df.alias("a").join(
            mt_eval_df.alias("b"),
            (col("a.security_id") == col("b.security_id")) &
            (col("a.customer_id") != col("b.customer_id")),
            "inner"
        ).select(
            col("a.customer_id").alias("customer_id_1"),
            col("a.security_id").alias("security_id_1"),
            col("a.buyorsell2").alias("buyorsell2_1"),
            col("a.agg_notional_value").alias("agg_notional_value_1"),
            col("b.customer_id").alias("customer_id_2"),
            col("b.security_id").alias("security_id_2"),
            col("b.buyorsell2").alias("buyorsell2_2"),
            col("b.agg_notional_value").alias("agg_notional_value_2")
        )

        mt_data_df = mt_data_df.withColumn("ratio_diff1", (col("agg_notional_value_1") / col("agg_notional_value_2")).cast("double")) \
                               .withColumn("ratio_diff2", (col("agg_notional_value_2") / col("agg_notional_value_1")).cast("double")) \
                               .withColumn("ratio_check", when((col("ratio_diff1") >= 0.9) & (col("ratio_diff1") <= 1.1), 1).otherwise(0)) \
                               .withColumn("ratio_check2", when((col("ratio_diff2") >= 0.9) & (col("ratio_diff2") <= 1.1), 1).otherwise(0)) \
                               .withColumn("rounded_ratio", round(col("ratio_diff1"), 4))

        mt_data_df = mt_data_df.filter(
            ((col("ratio_check") == 1) | (col("ratio_check2") == 1)) &
            (col("buyorsell2_1") != col("buyorsell2_2")) &
            ((col("agg_notional_value_1") > 5000) | (col("agg_notional_value_2") > 5000))
        )

        mt_data_df = mt_data_df.withColumn("rolling_period", concat(lit(current_start_str), lit("_to_"), lit(current_end_str)))

        final_df = final_df.unionByName(mt_data_df, allowMissingColumns=True)

    # === Post-filtering: Remove rows where no trade happened on end date for pair/security ===

    # Extract end date from rolling_period
    final_df = final_df.withColumn("rolling_end_date", split(col("rolling_period"), "_to_")[1].cast("date"))

    # Get valid trades on end date for either customer/security
    end_day_trades = df_filtered.filter(col("buyorsell2").isin(["BUY", "SELL"])) \
                                .select("customer_id", "security_id", "businessdate").distinct()

    # Join with final_df to keep only those rows where trade happened on end date
    final_df = final_df.join(
        end_day_trades,
        (
            ((final_df.customer_id_1 == end_day_trades.customer_id) & (final_df.security_id_1 == end_day_trades.security_id)) |
            ((final_df.customer_id_2 == end_day_trades.customer_id) & (final_df.security_id_2 == end_day_trades.security_id))
        ) &
        (final_df.rolling_end_date == end_day_trades.businessdate),
        "inner"
    ).drop("customer_id", "security_id", "businessdate", "rolling_end_date")

    # === Deduplicate by earliest rolling period per customer/security/ratio ===
    dedup_window = Window.partitionBy("customer_id_1", "customer_id_2", "security_id_1", "rounded_ratio") \
                         .orderBy("rolling_period")

    final_df = final_df.withColumn("row_num", row_number().over(dedup_window)) \
                       .filter(col("row_num") == 1) \
                       .drop("row_num")

    # Count instances
    instance_df = final_df.groupBy("customer_id_1", "customer_id_2").count() \
                          .withColumnRenamed("count", "instance_size") \
                          .withColumnRenamed("customer_id_1", "cid1") \
                          .withColumnRenamed("customer_id_2", "cid2")

    final_df = final_df.join(
        instance_df,
        (final_df.customer_id_1 == instance_df.cid1) &
        (final_df.customer_id_2 == instance_df.cid2),
        "left"
    ).drop("cid1", "cid2")

    # Final filters
    final_df = final_df.dropDuplicates()
    final_df = final_df.filter(col("instance_size") >= 3)
    final_df = final_df.filter(col("buyorsell2_1") == "BUY")

    df_filtered.unpersist()

    return final_df


