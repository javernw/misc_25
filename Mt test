from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, datediff, sum as _sum
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.master("local").appName("MirrorTrading").getOrCreate()

# Sample schema: customer_id, security_id, transaction_type (buy/sell), amount, date
# You can replace this with loading your data
data = [
    ("customer_a", "security_1", "buy", 1000, "2025-02-01"),
    ("customer_b", "security_1", "sell", 950, "2025-02-02"),
    ("customer_a", "security_1", "buy", 1200, "2025-02-04"),
    ("customer_b", "security_1", "sell", 1100, "2025-02-05"),
    # Add more sample data here
]

# Create DataFrame from data
df = spark.createDataFrame(data, ["customer_id", "security_id", "transaction_type", "amount", "date"])

# Convert date column to proper date format
df = df.withColumn("date", unix_timestamp("date", "yyyy-MM-dd").cast("timestamp"))

# Create a self-join to check for mirror trading conditions
# Join on security_id, but customers must not be the same
df_joined = df.alias("df1").join(df.alias("df2"),
                                 (df["security_id"] == df["df2.security_id"]) &
                                 (df["customer_id"] != df["df2.customer_id"]),
                                 "inner")

# Add a column to calculate the difference in dates for the same security_id
df_joined = df_joined.withColumn("date_diff", datediff("df2.date", "df1.date"))

# Filter the rows where the date difference is within 5 days
df_joined = df_joined.filter((col("date_diff") >= 0) & (col("date_diff") <= 5))

# Filter for opposite transaction types (buy/sell)
df_joined = df_joined.filter(((col("df1.transaction_type") == "buy") & (col("df2.transaction_type") == "sell")) |
                             ((col("df1.transaction_type") == "sell") & (col("df2.transaction_type") == "buy")))

# Calculate the notional ratio
df_joined = df_joined.withColumn("notional_ratio", 
                                 col("df1.amount") / col("df2.amount"))

# Filter for notional ratio between 90% and 110%
df_joined = df_joined.filter((col("notional_ratio") >= 0.9) & (col("notional_ratio") <= 1.1))

# For each customer, get the aggregate sum of amounts (buy or sell) within 5 days
df_aggregated = df_joined.groupBy("df1.customer_id", "df1.security_id", "df1.date") \
    .agg(
        _sum("df1.amount").alias("buy_amount"),
        _sum("df2.amount").alias("sell_amount")
    )

# Show the results
df_aggregated.show()

# Stop Spark session
spark.stop()


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, sum as _sum, unix_timestamp
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    # More rows can be added here
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to timestamp for easier manipulation
df = df.withColumn("trade_date", unix_timestamp(col("trade_date"), "yyyy-MM-dd").cast("timestamp"))

# Define window for 5-day lookback per customer and security
window_spec = Window.partitionBy("security_id", "customer_id").orderBy("trade_date")

# Add column for the 5-day lookback for aggregate sum
df = df.withColumn("five_day_window", F.sum("notional_value").over(window_spec.rangeBetween(-4, 0)))

# Self-join the DataFrame to compare each customer's trades with the other
mirror_trading_df = df.alias("df_a") \
    .join(df.alias("df_b"), (col("df_a.security_id") == col("df_b.security_id")) & 
          (col("df_a.customer_id") != col("df_b.customer_id")), "inner") \
    .filter((col("df_a.trade_date") <= col("df_b.trade_date")) & 
            (col("df_a.trade_date") >= (col("df_b.trade_date") - F.expr("INTERVAL 5 DAYS"))))

# Compute the absolute difference in notional value between customers A and B
mirror_trading_df = mirror_trading_df.withColumn("notional_diff", 
                                                 (col("df_a.notional_value") / col("df_b.notional_value")).cast("double"))

# Check if the notional value difference is within 90%-110% range
mirror_trading_df = mirror_trading_df.withColumn("notional_check", 
                                                 when((col("notional_diff") >= 0.9) & (col("notional_diff") <= 1.1), 1).otherwise(0))

# Filter for trades with 3 or more instances within the 30-day window
window_spec_30_day = Window.partitionBy("security_id", "customer_id").rangeBetween(-30 * 24 * 60 * 60, 0)
mirror_trading_df = mirror_trading_df.withColumn("count_within_30_days", 
                                                 F.count("df_a.trade_date").over(window_spec_30_day))

# Filter the results to find instances where customers are trading in opposite directions (buy/sell)
mirror_trading_df = mirror_trading_df.filter((col("df_a.trade_type") != col("df_b.trade_type")) & 
                                              (col("notional_check") == 1) & 
                                              (col("count_within_30_days") >= 3))

# Select relevant columns and show the results
result_df = mirror_trading_df.select("df_a.customer_id", "df_a.trade_date", "df_a.security_id", 
                                     "df_a.trade_type", "df_a.notional_value", "df_b.customer_id", 
                                     "df_b.trade_date", "df_b.trade_type", "df_b.notional_value")

# Display the result
result_df.show(truncate=False)


condition = (
    (col("currencyStatus1") != col("currencyStatus2")) & 
    (
        ((col("buyorsell2_1") == "SELL") | (col("buyorsell2_2") == "SELL")) & 
        ((col("currencyStatus1") == "restricted") | (col("currencyStatus2") == "restricted"))
    )
)


from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    ("customer_a", "2025-03-10", "SEC1", "buy", 1030),
    ("customer_b", "2025-03-10", "SEC1", "sell", 1010),
    ("customer_a", "2025-03-15", "SEC1", "buy", 1040),
    ("customer_b", "2025-03-15", "SEC1", "sell", 1025),
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to date format for comparison
df = df.withColumn("trade_date", F.to_date("trade_date", "yyyy-MM-dd"))

# Self-join on security_id to compare trades between different customers
mirror_trading_df = df.alias("df_a") \
    .join(df.alias("df_b"), 
          (F.col("df_a.security_id") == F.col("df_b.security_id")) & 
          (F.col("df_a.customer_id") != F.col("df_b.customer_id")), 
          "inner") \
    .filter(
        (F.col("df_a.trade_date") >= F.col("df_b.trade_date")) & 
        (F.col("df_a.trade_date") <= F.col("df_b.trade_date") + F.expr("INTERVAL 5 DAYS"))
    ) \
    .filter(F.col("df_a.trade_type") != F.col("df_b.trade_type"))  # Buy vs Sell

# Compute the ratio of notional values
mirror_trading_df = mirror_trading_df.withColumn("notional_ratio", 
                                                 (F.col("df_a.notional_value") / F.col("df_b.notional_value")).cast("double"))

# Check if the notional value ratio is within 90%-110%
mirror_trading_df = mirror_trading_df.filter(
    (F.col("notional_ratio") >= 0.9) & (F.col("notional_ratio") <= 1.1)
)

# Count trades per customer pair within 30 days
trade_count_df = mirror_trading_df.groupBy("df_a.security_id", "df_a.customer_id", "df_b.customer_id") \
    .agg(F.count("*").alias("mirror_trade_count"))

# Filter for customer pairs with at least 3 mirror trading instances
trade_count_df = trade_count_df.filter(F.col("mirror_trade_count") >= 3)

# Join back to get full details of those trades
final_result_df = mirror_trading_df.join(trade_count_df, 
                                         on=["security_id", "df_a.customer_id", "df_b.customer_id"], 
                                         how="inner")

# Select relevant columns for final output
final_result_df = final_result_df.select(
    "df_a.customer_id", "df_a.trade_date", "df_a.security_id",
    "df_a.trade_type", "df_a.notional_value", "df_b.customer_id",
    "df_b.trade_date", "df_b.trade_type", "df_b.notional_value",
    "mirror_trade_count"
)

# Show results
final_result_df.show(truncate=False)


from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingDetection").getOrCreate()

# Sample Data
data = [
    ("customer_a", "2025-03-01", "SEC1", "buy", 1000),
    ("customer_b", "2025-03-01", "SEC1", "sell", 950),
    ("customer_a", "2025-03-02", "SEC1", "buy", 1100),
    ("customer_b", "2025-03-02", "SEC1", "sell", 1050),
    ("customer_a", "2025-03-03", "SEC1", "buy", 1050),
    ("customer_b", "2025-03-03", "SEC1", "sell", 1000),
    ("customer_a", "2025-03-10", "SEC1", "buy", 1030),
    ("customer_b", "2025-03-10", "SEC1", "sell", 1010),
    ("customer_a", "2025-03-15", "SEC1", "buy", 1040),
    ("customer_b", "2025-03-15", "SEC1", "sell", 1025),
]

# Define schema and create DataFrame
columns = ["customer_id", "trade_date", "security_id", "trade_type", "notional_value"]
df = spark.createDataFrame(data, columns)

# Convert trade_date to date format
df = df.withColumn("trade_date", F.to_date("trade_date", "yyyy-MM-dd"))

# Aggregate sum of notional values within a 5-day rolling window for each customer & security
df_agg = df.alias("df1").join(
    df.alias("df2"),
    (F.col("df1.security_id") == F.col("df2.security_id")) &
    (F.col("df1.customer_id") == F.col("df2.customer_id")) &
    (F.col("df1.trade_date") >= F.col("df2.trade_date")) &
    (F.col("df1.trade_date") <= F.col("df2.trade_date") + F.expr("INTERVAL 5 DAYS")),
    "inner"
).groupBy("df1.customer_id", "df1.trade_date", "df1.security_id", "df1.trade_type") \
 .agg(F.sum("df1.notional_value").alias("aggregate_notional_value"))

# Self-join on security_id to find mirror trades within the 5-day window
mirror_trading_df = df_agg.alias("df_a") \
    .join(df_agg.alias("df_b"), 
          (F.col("df_a.security_id") == F.col("df_b.security_id")) & 
          (F.col("df_a.customer_id") != F.col("df_b.customer_id")), 
          "inner") \
    .filter(
        (F.col("df_a.trade_date") >= F.col("df_b.trade_date")) & 
        (F.col("df_a.trade_date") <= F.col("df_b.trade_date") + F.expr("INTERVAL 5 DAYS"))
    ) \
    .filter(F.col("df_a.trade_type") != F.col("df_b.trade_type"))  # Buy vs Sell

# Compute the ratio of aggregate notional values
mirror_trading_df = mirror_trading_df.withColumn("notional_ratio", 
    (F.col("df_a.aggregate_notional_value") / F.col("df_b.aggregate_notional_value")).cast("double"))

# Check if the notional value ratio is within 90%-110%
mirror_trading_df = mirror_trading_df.filter(
    (F.col("notional_ratio") >= 0.9) & (F.col("notional_ratio") <= 1.1)
)

# Count trades per customer pair within 30 days
trade_count_df = mirror_trading_df.groupBy("df_a.security_id", "df_a.customer_id", "df_b.customer_id") \
    .agg(F.count("*").alias("mirror_trade_count"))

# Filter for customer pairs with at least 3 mirror trading instances
trade_count_df = trade_count_df.filter(F.col("mirror_trade_count") >= 3)

# Join back to get full details of those trades
final_result_df = mirror_trading_df.join(trade_count_df, 
                                         on=["security_id", "df_a.customer_id", "df_b.customer_id"], 
                                         how="inner")

# Select relevant columns for final output
final_result_df = final_result_df.select(
    "df_a.customer_id", "df_a.trade_date", "df_a.security_id",
    "df_a.trade_type", "df_a.aggregate_notional_value", "df_b.customer_id",
    "df_b.trade_date", "df_b.trade_type", "df_b.aggregate_notional_value",
    "mirror_trade_count"
)

# Show results
final_result_df.show(truncate=False)

from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark Session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingLoop").getOrCreate()

# Sample list of date ranges (each tuple represents a start and end date)
date_ranges = [
    ("2024-08-18", "2024-08-23"),
    ("2024-08-25", "2024-08-30"),
    ("2024-09-01", "2024-09-05")
]

# Placeholder list to store results from each iteration
results = []

# Loop through each date range and execute the query
for start_date, end_date in date_ranges:
    # Filter data within the current date range
    mt_eval_df = df.filter(F.col("security_id") == "H574748") \
                   .filter(F.col("businessdate").between(start_date, end_date)) \
                   .filter(F.col("IDAgg").isin([1010, 1005])) \
                   .groupBy("IDAgg", "security_id", "buyorsell2") \
                   .agg(F.sum("amountusd").alias("agg_notional_value"))

    # Self-join for mirror trade evaluation
    mt_data_df = mt_eval_df.alias("MT_a").join(
        mt_eval_df.alias("MT_b"),
        (F.col("MT_a.security_id") == F.col("MT_b.security_id")) &
        (F.col("MT_a.IDAgg") != F.col("MT_b.IDAgg")),
        "inner"
    )

    # Append the result to the list
    results.append(mt_data_df)

# Union all results into one DataFrame
final_df = results[0]  # Start with the first result
for df in results[1:]:
    final_df = final_df.union(df)  # Append subsequent results

# Show final output
final_df.show()



from pyspark.sql import SparkSession
import pyspark.sql.functions as F

# Initialize Spark Session
spark = SparkSession.builder.master("local[*]").appName("MirrorTradingLoop").getOrCreate()

# List of customer pairs and their respective unique date ranges
customer_pairs = ["cust1_cust2", "cust3_cust4", "cust5_cust6"]

# Assign unique date ranges to each customer pair dynamically
customer_date_ranges = {
    tuple(pair.split("_")): date_range for pair, date_range in zip(
        customer_pairs, 
        [("2024-08-18", "2024-08-23"), ("2024-09-01", "2024-09-05"), ("2024-09-20", "2024-09-25")]  # Unique date ranges
    )
}

# Placeholder list to store results
results = []

# Loop through each customer pair and their respective date range
for (customer1, customer2), (start_date, end_date) in customer_date_ranges.items():
    
    # Filter data within the current customer pair and their date range
    mt_eval_df = df.filter(
        (F.col("customer_id").isin([customer1, customer2])) &  # Match the customer pair
        (F.col("businessdate").between(start_date, end_date))  # Match their assigned date range
    ).groupBy("customer_id", "security_id", "buyorsell2") \
     .agg(F.sum("amountusd").alias("agg_notional_value"))

    # Self-join for mirror trade evaluation within the same security
    mt_data_df = mt_eval_df.alias("MT_a").join(
        mt_eval_df.alias("MT_b"),
        (F.col("MT_a.security_id") == F.col("MT_b.security_id")) &
        (F.col("MT_a.customer_id") != F.col("MT_b.customer_id")),  # Ensuring it's a trade between different customers
        "inner"
    ).withColumn("customer_pair", F.lit(f"{customer1} & {customer2}")) \
     .withColumn("date_range", F.lit(f"{start_date} to {end_date}"))

    # Append the result to the list
    results.append(mt_data_df)

# Union all results into one DataFrame if results exist
if results:
    final_df = results[0]  # Start with the first result
    for df in results[1:]:
        final_df = final_df.union(df)  # Append subsequent results

    # Show final output
    final_df.show(truncate=False)
else:
    print("No matching data found for the given customer pairs and date ranges.")


