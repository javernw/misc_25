import pandas as pd
import gensim
from gensim import corpora
from gensim.models import CoherenceModel, LdaModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import itertools

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load the data
input_file = 'data.csv'  # Path to the CSV file
text_column = 'text'     # Column name containing the text
df = pd.read_csv(input_file)

# Preprocess the text data
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())
    # Remove stop words and non-alphabetic tokens
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return tokens

processed_texts = [preprocess_text(text) for text in df[text_column].dropna()]

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(processed_texts)
corpus = [dictionary.doc2bow(text) for text in processed_texts]

# Function to compute coherence score for LDA model
def compute_coherence_score(corpus, dictionary, num_topics, alpha, eta):
    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, alpha=alpha, eta=eta, random_state=0)
    coherence_model = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')
    coherence_score = coherence_model.get_coherence()
    return coherence_score

# Parameter grid
num_topics_list = [5, 10, 15]  # Number of topics to try
alpha_list = ['symmetric', 'asymmetric']  # Alpha parameter
eta_list = ['symmetric', None]  # Eta parameter

# Perform grid search
best_coherence = 0
best_params = None

for num_topics, alpha, eta in itertools.product(num_topics_list, alpha_list, eta_list):
    coherence_score = compute_coherence_score(corpus, dictionary, num_topics, alpha, eta)
    print(f"Num Topics: {num_topics}, Alpha: {alpha}, Eta: {eta}, Coherence Score: {coherence_score:.4f}")
    
    if coherence_score > best_coherence:
        best_coherence = coherence_score
        best_params = (num_topics, alpha, eta)

# Print the best parameters
print("\nBest Model Parameters:")
print(f"Number of Topics: {best_params[0]}")
print(f"Alpha: {best_params[1]}")
print(f"Eta: {best_params[2]}")
print(f"Best Coherence Score: {best_coherence:.4f}")



import pandas as pd

def format_topics_sentences(ldamodel, corpus, texts):
    # Initialize output dataframe
    sent_topics_df = pd.DataFrame()

    # Get the main topic for each document
    for i, row in enumerate(ldamodel[corpus]):
        if isinstance(row, list):  # Check if row is a list
            row = sorted(row, key=lambda x: (x[1]), reverse=True)
            # Get the Dominant topic, Perc Contribution, and Keywords for each document
            for j, (topic_num, prop_topic) in enumerate(row):
                if j == 0:  # The dominant topic
                    wp = ldamodel.show_topic(topic_num)
                    topic_keywords = ", ".join([word for word, prop in wp])
                    sent_topics_df = sent_topics_df.append(
                        pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),
                        ignore_index=True
                    )
                else:
                    break
        else:
            # If row is not a list, handle the case accordingly
            topic_num, prop_topic = row
            wp = ldamodel.show_topic(topic_num)
            topic_keywords = ", ".join([word for word, prop in wp])
            sent_topics_df = sent_topics_df.append(
                pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]),
                ignore_index=True
            )

    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add the original text to the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return sent_topics_df

# Example usage
df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)

# Format the dataframe
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Display the result
df_dominant_topic.head(10)



import pandas as pd

def find_dominant_topic(ldamodel, corpus, texts):
    """
    Finds the dominant topic in each document based on the highest probability.

    Parameters:
        ldamodel: Trained LDA model.
        corpus: Corpus used for training the LDA model.
        texts: List of original text documents.

    Returns:
        DataFrame with the dominant topic, its percentage contribution, keywords, and the original text.
    """
    # Initialize a list to store the results
    dominant_topics = []

    # Iterate through each document's topic distribution
    for i, row in enumerate(ldamodel[corpus]):
        # Filter to ensure each element in row is a tuple of length 2
        row = [item for item in row if isinstance(item, tuple) and len(item) == 2]

        # Sort the topics in descending order of probability
        if len(row) > 0:
            row = sorted(row, key=lambda x: x[1], reverse=True)

            # Get the dominant topic, its probability, and the topic's keywords
            dominant_topic_num, topic_prob = row[0]
            topic_keywords = ", ".join([word for word, prop in ldamodel.show_topic(dominant_topic_num)])

            # Append the results to the list
            dominant_topics.append((i, int(dominant_topic_num), round(topic_prob, 4), topic_keywords, texts[i]))

    # Create a DataFrame with the results
    df_dominant_topics = pd.DataFrame(dominant_topics, columns=['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'])

    return df_dominant_topics

# Example usage
df_dominant_topic = find_dominant_topic(ldamodel=optimal_model, corpus=corpus, texts=data)

# Display the result
df_dominant_topic.head(10)



import pandas as pd

def find_dominant_topic(ldamodel, corpus, texts):
    """
    Finds the dominant topic in each document based on the highest probability.

    Parameters:
        ldamodel: Trained LDA model.
        corpus: Corpus used for training the LDA model.
        texts: List of original text documents.

    Returns:
        DataFrame with the dominant topic, its percentage contribution, keywords, and the original text.
    """
    # Initialize a list to store results
    dominant_topics = []

    # Iterate through each document's topic distribution
    for i, row in enumerate(ldamodel[corpus]):
        if row:  # Ensure row is not empty
            # Sort topics in descending order of probability
            row = sorted(row, key=lambda x: x[1], reverse=True)

            # Get the dominant topic, probability, and keywords
            dominant_topic_num, topic_prob = row[0]  # Most relevant topic
            topic_keywords = ", ".join([word for word, prop in ldamodel.show_topic(dominant_topic_num)])

            # Append the results to the list
            dominant_topics.append((i, int(dominant_topic_num), round(topic_prob, 4), topic_keywords, texts[i]))
        else:
            # Handle case where no topic is assigned
            dominant_topics.append((i, None, None, "No Topic", texts[i]))

    # Create a DataFrame with results
    df_dominant_topics = pd.DataFrame(dominant_topics, columns=['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'])

    return df_dominant_topics

# Example usage
df_dominant_topic = find_dominant_topic(ldamodel=optimal_model, corpus=corpus, texts=data)

# Display the first 10 results
df_dominant_topic.head(10)



import pandas as pd

def find_dominant_topic(ldamodel, corpus, texts):
    """
    Finds the dominant topic in each document based on the highest probability.

    Parameters:
        ldamodel: Trained LDA model.
        corpus: Corpus used for training the LDA model.
        texts: List of original text documents.

    Returns:
        DataFrame with the dominant topic, its percentage contribution, keywords, and the original text.
    """
    # Initialize a list to store results
    dominant_topics = []

    # Iterate through each document in the corpus
    for i, bow in enumerate(corpus):
        # Get the topic distribution for the document
        topics = ldamodel.get_document_topics(bow, minimum_probability=0)  # Ensures all topics get a probability

        if topics:  # Ensure topics exist for this document
            # Sort topics in descending order of probability
            dominant_topic_num, topic_prob = max(topics, key=lambda x: x[1])

            # Get the top words for the dominant topic
            topic_keywords = ", ".join([word for word, _ in ldamodel.show_topic(dominant_topic_num)])

            # Store results
            dominant_topics.append((i, int(dominant_topic_num), round(topic_prob, 4), topic_keywords, texts[i]))
        else:
            # Handle case where no topics are assigned
            dominant_topics.append((i, None, None, "No Topic", texts[i]))

    # Create a DataFrame with results
    df_dominant_topics = pd.DataFrame(dominant_topics, columns=['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'])

    return df_dominant_topics

# Example usage
df_dominant_topic = find_dominant_topic(ldamodel=optimal_model, corpus=corpus, texts=data)

# Display the first 10 results
df_dominant_topic.head(10)


import pandas as pd

# Initialize an empty DataFrame to store the most representative document for each topic
most_representative_docs = pd.DataFrame()

# Group by 'Dominant_Topic' to get the most representative document per topic
topic_groups = df_dominant_topic.groupby('Dominant_Topic')

for topic, group in topic_groups:
    # Select the document with the highest topic percentage contribution
    most_representative_docs = pd.concat([most_representative_docs, 
                                          group.sort_values(['Topic_Perc_Contrib'], ascending=False).head(1)], 
                                         axis=0)

# Reset index
most_representative_docs.reset_index(drop=True, inplace=True)

# Format column names
most_representative_docs.columns = ['Topic_Num', 'Topic_Perc_Contrib', 'Keywords', 'Text']

# Show the most representative document per topic
print(most_representative_docs.head())





def topic_diversity(lda_model, top_n=10):
    # Get all top N words from each topic
    top_words_per_topic = [word for topic_id in range(lda_model.num_topics) for word, _ in lda_model.show_topic(topic_id, topn=top_n)]
    unique_top_words = set(top_words_per_topic)
    
    # Calculate topic diversity
    diversity = len(unique_top_words) / len(top_words_per_topic)
    return diversity

# Calculate topic diversity for the top 10 words
diversity = topic_diversity(lda_model, top_n=10)
print(f"Topic Diversity: {diversity:.2f}")


