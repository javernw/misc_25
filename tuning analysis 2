
Here’s a well-structured paragraph for the “Security Issuer with Few Employees” score, reflecting your lower-tail analysis attempt and SME-driven threshold selection:

⸻

To assess the risk associated with security issuers operating with unusually low employee counts, we applied a methodology similar to that used in mirror trading—filtering the dataset, removing the upper quartile to reduce skew, and attempting to derive a meaningful lower-bound threshold. Since this score is driven by a “less than or equal to” condition, the focus was on identifying meaningful cutoffs in the lower tail of the employee count distribution. However, during analysis, the data showed a sharp jump between the 0th and 1st percentile (e.g., from 0 to 100 employees), with no meaningful gradation in between. This irregular distribution made percentile-based thresholding ineffective, as it failed to capture the subtlety of legitimate low-operating entities versus shell-like profiles. Given these limitations, the final threshold was determined using subject matter expert (SME) discretion, based on known operational profiles and prior case experience. This ensured the score remains risk-aligned, targeting issuers with unusually lean structures that may warrant further investigation.


Certainly — here’s a parallel paragraph for Mirror Trading, incorporating your methodology:

⸻

To establish a threshold for detecting mirror trading behavior, we used a dataset built on a 3-day rolling window that captures potential matched trade patterns between counterparties. Within this windowed dataset, we filtered for trade pairs with notional ratios between 90% and 110%, ensuring that only trades with near-identical economic exposure were considered. This filtering step was critical to isolate candidate mirror trades that reflect true alignment in notional size, as wider deviations may reflect coincidental or non-risk-aligned activity. From this subset, we removed the upper quartile to eliminate extreme values that could skew the distribution. We then calculated the 50th percentile of relevant metrics (e.g., trade value or frequency) to determine a baseline threshold for further analysis. This threshold, chosen in alignment with SME input, helps differentiate between opportunistic alignment and deliberate patterning—capturing mirror trades that are both proportionally matched and consistent enough to warrant further scrutiny.



Here’s the updated paragraph with all your context incorporated:

⸻

To identify customers trading significantly above their historical profile, we compared current month activity to each customer’s historical average or median across a defined lookback window. We calculated the ratio of current-to-historical activity for both trade value and volume, then analyzed the distribution of these ratios across the population. To ensure a meaningful baseline, we first filtered the dataset to include only records with positive Z-scores, focusing on customers whose current month activity was above their historical norm. Additionally, we removed only the upper quartile of extreme values to reduce skew without discarding relevant mid-tier behavior. Percentile analysis was then applied to the filtered set. Based on this, the 50th percentile of current month value and volume (within the positively flagged group) was selected as a minimum threshold, informed by SME judgment. This floor ensures that only customers demonstrating both elevated and operationally significant activity are captured. The final threshold reflects both statistical support and practical business context for defining “Trading Above Profile.”


from pyspark.sql.functions import col

# Define your percentiles
percentiles = [0.75, 0.90, 0.95]

# Compute percentile thresholds on the IQR-filtered data
thresholds = df_no_outliers.approxQuantile("aggregate_notional", percentiles, 0.01)

# Zip them into a list of (percentile, threshold)
percentile_thresholds = list(zip(percentiles, thresholds))

# Filter original DataFrame for each threshold and count how many rows meet or exceed it
for pct, threshold in percentile_thresholds:
    count = df_original.filter(col("aggregate_notional") >= threshold).count()
    print(f"{int(pct * 100)}th percentile threshold: {threshold} → Rows >= threshold: {count}")


# Step 3: Compute IQR for aggregate_notional
iqr_bounds = df.approxQuantile("aggregate_notional", [0.25, 0.75], 0.01)
q1, q3 = iqr_bounds
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Step 4: Filter out outliers
df_no_outliers = df.filter((col("aggregate_notional") >= lower_bound) & (col("aggregate_notional") <= upper_bound))

# Step 5: Show cleaned data
df_no_outliers.show(truncate=False)

# Optional: Recalculate percentiles on cleaned data
percentiles = [0.5, 0.75, 0.9, 0.95, 0.99]
agg_notional_percentiles_cleaned = df_no_outliers.select(
    percentile_approx("aggregate_notional", percentiles, 100).alias("aggregate_notional_percentiles")
)
agg_notional_percentiles_cleaned.show()


Days):
Mirror trading typically involves trades executed in close succession—often on the same day or within a very short period—to avoid market exposure and minimize detection. Reducing the detection window from 5 days to 3 days increases precision by focusing on more tightly aligned trading behavior, which is more indicative of coordinated or intentional mirror activity. This adjustment helps reduce false positives from unrelated trades that happen to fall within a wider time frame, while still capturing the core risk behavior characterized by near-simultaneous buy-sell matches.


Here’s a professional write-up of rationales for your selected thresholds, based on the three scenarios in the tuning matrix and assuming you used distribution analysis (e.g., KDE plots, IQR filtering, z-scores, percentile thresholds) and domain considerations:

⸻

Scenario 1: Acting Out of Line with Peer Group

Thresholds:
	•	FX/EQ Value STD (Severity 1 & 2): 8, 5
	•	FX/EQ Volume STD (Severity 1 & 2): 8, 5

Rationale:
	•	Z-score-based peer deviation was used to measure how far a customer trades above or below their peer group.
	•	KDE analysis showed most customers clustered below z-score of 1, with steep drop-offs around 2–3.
	•	By choosing a threshold of 8, the escalation tier targets extreme statistical deviations (99.9th percentile+), likely indicative of deliberate or high-impact behavior divergence.
	•	The Severity 2 thresholds (e.g., 5) provide an early warning buffer, catching significant but less extreme deviations.
	•	This dual threshold structure aligns with the goal of both risk alerting and tuning sensitivity.

⸻

Scenario 2: Trading Above Expected Profiles

Thresholds:
	•	TBD for value/volume STD (pending validation)
	•	Min Aggregated Value: 5000
	•	Min Aggregated Volume: 2

Rationale:
	•	This scenario filters customers who exceed their expected trading range, as defined by profile norms or historical behavior.
	•	The minimum aggregated thresholds (value ≥ 5000, volume ≥ 2) ensure that low-activity noise is excluded and only repeat behavior or meaningful value trades are flagged.
	•	Lookback periods (12 months trading history, 3-month minimum activity, 12-month KYC onboarding) were chosen to ensure stable baselines for comparison.
	•	Value and volume STD thresholds will be finalized after percentile/z-score review on updated post-IQR datasets.

⸻

Scenario 3: Customer Trades in a Restricted / High-Risk Non-Home Currency

Thresholds:
	•	High-Risk Currency Value: $31M
	•	High-Risk Currency Volume: 10
	•	Restricted Currency Value: $50M
	•	Restricted Currency Volume: 14

Rationale:
	•	KDE plots and IQR analysis were used to remove fat-finger trades, revealing the true distribution of restricted/high-risk currency trading.
	•	Thresholds are set around the 95–98th percentile, balancing false positives and risk exposure.
	•	The volume threshold (10–14) ensures that the trade pattern is sustained rather than incidental.
	•	Value thresholds ($31M, $50M) were derived from inflection points observed in the tail of the distribution curves, consistent with where risk signals tend to cluster.
	•	These levels prioritize escalation for meaningful risk exposure while filtering out operational noise.

⸻

Let me know if you’d like these formatted for a slide deck or presentation talking points!


Great — here’s how you could structure your approach document with the rationales integrated into a professional format.

⸻

Scoring Framework Approach Document

1. Overview

This document outlines the approach used to develop and apply scoring logic to detect potentially suspicious trading behaviors. Each score represents either a scenario-based indicator or a customer-level risk factor. Scores are assigned based on severity, weighted contribution to alerts, and aggregated to support decision-making.

⸻

2. Scoring Design

Each score consists of:
	•	Severity: Reflects the seriousness of the behavior detected (e.g., 100% for critical indicators, 50% for supporting indicators).
	•	Weight: Represents how much each score contributes to a given alert type, ensuring fair distribution when multiple scores apply.
	•	Contribution: Product of severity and weight.
	•	Rationale: Explanation of why the score is used and how it contributes to identifying risk.

⸻

3. Scenarios and Risk Factors

Score Name	Rationale
Mirror Trading	Captures coordinated buy/sell activity between accounts trading the same security in opposite directions, suggesting collusion or fund obfuscation. Given its direct link to potential money laundering, it carries full weight and severity.
Acting Out of Line with Peer Group	Identifies customers whose trading behavior (volume/value) deviates significantly from peers. These deviations may signal unusual or anomalous trading patterns. Given its supportive nature, it contributes partially when combined with other factors.
Trading Above Expected Profiles	Detects accounts trading above their historical or expected profile, which may indicate sudden risk escalation or unauthorized activity. Weighted partially to reflect context.
Customers With Hidden Relationship Trading Same Security	Flags unlinked accounts engaging in similar trades, suggesting undisclosed relationships or potential collusion. Due to the seriousness of hidden affiliations, it carries full weight and severity.
Customer On Watchlist	Captures transactions involving customers listed on regulatory or internal watchlists, representing elevated baseline risk. Weighted moderately to ensure it’s considered alongside contextual behavior.
High Risk Customers	Represents inherent customer risk based on attributes such as geography, entity type, or regulatory classification. Weighted equally with other factors when combined.
Customer Trades in Restricted / High-Risk Non-Home Currency	Detects trades involving restricted or high-risk currencies not associated with the customer’s jurisdiction, suggesting attempts to bypass controls or obscure fund movement. Assigned full contribution due to its risk implications.
Security Issuer from High-Risk Country	Highlights exposure to securities issued in jurisdictions with elevated AML risk. Carries moderate weight, especially relevant when combined with suspicious customer behavior.
Security Issuer Unaudited	Flags trades in securities issued by unaudited entities, which may signal lack of transparency or increased fraud risk. Weighted moderately to contribute in combination.
Security Issuer with Few Employees	Identifies issuers with limited operational footprint, often used as shell entities. On its own it’s low-risk, but when paired with other flags, it adds meaningful context.


⸻

4. Alert Contribution Logic
	•	If only one score applies to an alert, it is assigned 100% weight.
	•	If two scores apply, each is weighted at 50%.
	•	If three or more apply, weights are evenly distributed.
	•	Severity levels are fixed based on the nature of the indicator.

⸻

5. Conclusion

This scoring framework is designed to balance customer-level risk factors with transactional behavior. It supports the identification of complex suspicious patterns by weighting and combining individual indicators to form actionable alerts.

⸻

Let me know if you want this turned into a formatted Word or Markdown version.

Certainly — here’s a refined and professional version that clarifies the difference between scenarios and risk factors, and incorporates your explanation into the framework:

⸻

6. Scenarios vs. Risk Factors

The scoring model distinguishes between scenarios and risk factors based on their function in alert generation:
	•	Scenarios reflect observed transactional behavior or patterns detected during trading activity (e.g., mirror trading, trading above profile).
	•	Risk factors represent inherent or static attributes of a customer that elevate their baseline risk level (e.g., being on a watchlist, designated high-risk).

Scenarios are the primary drivers of alert generation, while risk factors act as enhancers, increasing the likelihood of triggering an alert when they co-occur with moderate-risk scenarios.

Enhanced Monitoring Mechanism:

Risk factors are particularly important for contextual escalation. For example:

If a customer is not designated as high risk, but they trigger two moderate scenarios weighted at 50 and 25, the total contribution of 75 may not meet the alert threshold. However, if the customer is classified as high risk (with a weight of 35), the combined contribution becomes 110 — thereby exceeding the threshold and generating an alert.

This mechanism ensures customers with elevated inherent risk receive additional scrutiny, especially when their behavior alone may not be sufficient to raise a red flag.

⸻

Let me know if you’d like a diagram or visual to accompany this section, or if you need the document broken out into slides or a report format.

Here’s a clear, structured write-up you can use to explain the table in a report or slide deck:

⸻

Threshold Summary Table Explanation

This table provides a breakdown of alert activity at the 85th, 90th, and 95th percentile thresholds for both value and volume metrics, across two risk categories: HighRisk and Restricted. The purpose of this table is to quantify how many activities (i.e., records) would be triggered if thresholds were set at these percentile levels, using AND logic — meaning both value and volume must exceed their respective thresholds.

⸻

Columns Explained:
	•	Group: The risk category being analyzed (HighRisk or Restricted)
	•	Total: Total number of activities evaluated for the group
	•	Value/Volume Xth Threshold: The actual numeric thresholds at the 85th, 90th, and 95th percentiles for value and volume metrics, based on IQR-cleaned data
	•	Activity > Xth: Count of activities where both value and volume exceeded the percentile thresholds
	•	% > Xth: Percentage of total activities that triggered based on both metrics exceeding the threshold

⸻

How to Read the Table:
	•	HighRisk Group Example:
	•	At the 85th percentile, the thresholds are:
	•	Value: $16.2M
	•	Volume: 5
	•	357 activities exceeded both thresholds (12.6% of the population)
	•	At 95th percentile, the thresholds rise to:
	•	Value: $31.3M
	•	Volume: 10
	•	214 activities exceeded both thresholds (7.6%)
	•	Restricted Group Example:
	•	Thresholds and volumes are generally higher due to the group’s profile.
	•	At the 95th percentile, only 81 activities triggered alerts (2.9%), reflecting tighter concentration in extreme cases.

⸻

What This Tells Us:
	•	As thresholds increase from 85th → 95th, fewer alerts are generated, allowing for more selective prioritization.
	•	The HighRisk group consistently triggers more alerts than the Restricted group, suggesting a wider range of elevated activity.
	•	These thresholds can be used to define risk tiers or alert categories (e.g., Moderate = 85th, High = 90th, Critical = 95th).

⸻

Would you like a version of this explanation tailored for presenting to non-technical stakeholders or compliance teams?

Here’s a simplified and business-friendly version of the explanation, suitable for non-technical stakeholders or compliance audiences:

⸻

How to Read the Threshold Summary Table (Business View)

This table helps us understand how many activities would be flagged for review if we set risk thresholds at different levels — specifically the 85th, 90th, and 95th percentiles — for both transaction value and volume.

We looked at two categories:
	•	HighRisk: Transactions involving high-risk non-home currencies
	•	Restricted: Transactions involving restricted currencies

⸻

Key Points:
	•	Thresholds: For each group, we show the cutoff values for trade value and volume at the 85th, 90th, and 95th percentiles. These values were calculated after cleaning the data to remove outliers.
	•	Activity Counts: We then count how many activities exceed both the value and volume thresholds at each level — meaning they are significantly high in both dimensions.
	•	Percent of Total: The table also shows what percentage of the total population those alerts represent.

⸻

Example (HighRisk group):
	•	If we set the threshold at the 90th percentile, we would flag:
	•	Transactions over $22.5M in value
	•	And with volume over 7
	•	This would trigger 273 alerts, or 9.7% of all HighRisk activities.
	•	At the 95th percentile, we raise the bar to:
	•	Over $31.3M in value
	•	Volume over 10
	•	That reduces alerts to 214 (just 7.6% of the group), helping to focus only on the most severe cases.

⸻

Why This Matters:
	•	This approach allows us to set data-driven thresholds — not arbitrary ones — based on actual transaction behavior.
	•	It gives us flexibility to tune alert volumes depending on the risk appetite or operational capacity.
	•	We can tier the alerting strategy (e.g., 85th = moderate, 95th = critical) to align with review priorities.

⸻

Would you like me to prepare this as a slide with visual bullet callouts and example charts?

