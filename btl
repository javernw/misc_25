# If we didnâ€™t hit the target size, top up randomly
    shortfall = requested_n - len(strat_sample)
    if shortfall > 0:
        remaining = df_unique[~df_unique['customer_id'].isin(strat_sample['customer_id'])]
        top_up = remaining.sample(n=min(shortfall, len(remaining)), random_state=42)
        strat_sample = pd.concat([strat_sample, top_up]

import pandas as pd

# Example: your datasets list
datasets = [df1, df2, df3, df4, df5, df6, df7]
sample_sizes = [10, 15, 20, 10, 100, 20, 25]  # Example sample targets
dataset_labels = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7']

used_ids = set()
samples = []

for i, (df, requested_n) in enumerate(zip(datasets, sample_sizes)):
    label = dataset_labels[i]

    # Make sure 'date' column is datetime
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    
    # Create month_year column
    df['month_year'] = df['date'].dt.to_period('M').astype(str)

    if len(df) < 100:
        # Small dataset: keep all rows
        sample = df.copy()
        sample['source_dataset'] = label
        samples.append(sample)
        continue

    # Large dataset: enforce no repeated customer_id across large datasets
    df_filtered = df[~df['customer_id'].isin(used_ids)]

    # Drop duplicate customer_id, keep 1 row per ID randomly
    df_shuffled = df_filtered.sample(frac=1, random_state=42)
    df_unique = df_shuffled.drop_duplicates(subset='customer_id')

    # Stratified sampling by month_year
    month_counts = df_unique['month_year'].value_counts(normalize=True)

    month_samples = []
    for month, proportion in month_counts.items():
        n_month = int(round(requested_n * proportion))
        
        df_month = df_unique[df_unique['month_year'] == month]
        
        # Ensure no oversampling
        n_month = min(n_month, len(df_month))
        
        if n_month > 0:
            month_sample = df_month.sample(n=n_month, random_state=42)
            month_samples.append(month_sample)

    # Combine stratified month samples
    if month_samples:
        sample = pd.concat(month_samples)
    else:
        sample = pd.DataFrame(columns=df.columns)

    # Add source label
    sample['source_dataset'] = label

    # Update used IDs
    used_ids.update(sample['customer_id'])

    samples.append(sample)

# Combine all samples
final_sample = pd.concat(samples, ignore_index=True)

# Output shape summary
print(final_sample.shape)
print(final_sample['source_dataset'].value_counts())


import pandas as pd

# Example list of DataFrames and sample sizes
datasets = [df1, df2, df3, df4, df5, df6, df7]
sample_sizes = [10, 15, 20, 10, 100, 20, 25]
dataset_labels = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7']

used_ids = set()
samples = []

for i, (df, requested_n) in enumerate(zip(datasets, sample_sizes)):
    label = dataset_labels[i]
    
    # Case 1: Small dataset, keep all rows regardless of duplicates
    if len(df) < 100:
        sample = df.copy()
        sample['source_dataset'] = label
        samples.append(sample)
        continue  # Do NOT update used_ids, allow overlaps
       
    # Case 2: Larger dataset - enforce unique IDs globally
    df_filtered = df[~df['customer_id'].isin(used_ids)]
    df_shuffled = df_filtered.sample(frac=1, random_state=42)
    df_unique = df_shuffled.drop_duplicates(subset='customer_id')

    sample = df_unique.sample(n=min(requested_n, len(df_unique)), random_state=42)
    sample['source_dataset'] = label

    used_ids.update(sample['customer_id'])
    samples.append(sample)

# Final combined sample
final_sample = pd.concat(samples, ignore_index=True)

# Optional check: only for large datasets should customer_id be unique
print(final_sample.shape)
print(final_sample['source_dataset'].value_counts())


import pandas as pd

# List of DataFrames
datasets = [df1, df2, df3, df4, df5, df6, df7]
sample_sizes = [10, 15, 20, 10, 100, 20, 25]
dataset_labels = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7']

used_ids = set()
samples = []

for i, (df, requested_n) in enumerate(zip(datasets, sample_sizes)):
    label = dataset_labels[i]

    # Step 1: Remove rows with already used customer_ids
    df_filtered = df[~df['customer_id'].isin(used_ids)]

    # Step 2: Drop duplicates to get one row per customer_id (random by shuffling first)
    df_shuffled = df_filtered.sample(frac=1, random_state=42)
    df_unique = df_shuffled.drop_duplicates(subset='customer_id')

    # Step 3: Decide how many rows to keep
    if len(df_unique) < 100:
        sample = df_unique
    else:
        sample = df_unique.sample(n=min(requested_n, len(df_unique)), random_state=42)

    # Step 4: Add identifier column
    sample['source_dataset'] = label

    # Step 5: Update used_ids with customer_ids from the **filtered DataFrame**
    # to prevent future overlap
    used_ids.update(df_unique['customer_id'])

    # Step 6: Store sample
    samples.append(sample)

# Combine all into one DataFrame
final_sample = pd.concat(samples, ignore_index=True)

# Final check (optional)
assert final_sample['customer_id'].duplicated().sum() == 0, "Duplicate customer_id found!"

print(final_sample.shape)
print(final_sample['source_dataset'].value_counts())

import pandas as pd

# Example: list of datasets
datasets = [df1, df2, df3, df4, df5, df6, df7]
sample_sizes = [10, 15, 20, 10, 100, 20, 25]  # example, total 200

# Set to track already used customer_ids
used_ids = set()

# List to collect sampled DataFrames
samples = []

for i, (df, n) in enumerate(zip(datasets, sample_sizes)):
    # Remove rows with IDs already used in previous datasets
    df_filtered = df[~df['customer_id'].isin(used_ids)]

    # Shuffle to randomize before deduplication
    df_shuffled = df_filtered.sample(frac=1, random_state=42)

    # Drop duplicates to get one row per customer_id
    df_unique = df_shuffled.drop_duplicates(subset='customer_id')

    # Sample from the remaining
    sample = df_unique.sample(n=min(n, len(df_unique)), random_state=42)

    # Track used customer_ids
    used_ids.update(sample['customer_id'])

    # Add to result list
    samples.append(sample)

# Final combined sample
final_sample = pd.concat(samples, ignore_index=True)

print(final_sample.shape)